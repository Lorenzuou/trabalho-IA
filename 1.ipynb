{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous pooling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data will be stored in this dataframe, with the method name, mean accuracy, standard deviation, lower and upper bound\n",
    "df = pd.DataFrame(columns=['method', 'mean', 'std', 'lower', 'upper'])\n",
    "\n",
    "df_per_fold = pd.DataFrame(columns=['method', 'fold', 'mean','std', 'lower', 'upper'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X.csv')\n",
    "y = pd.read_csv('y.csv')\n",
    "\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero Rule Baseline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "df = df.append({'method': 'ZR', 'mean': scores.mean(), 'std': scores.std(), 'lower': scores.mean() - scores.std(), 'upper': scores.mean() + scores.std()}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,params_grid,name, df): \n",
    "    cv_inner = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=36851234)\n",
    "    cv_outer = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "    pipe = Pipeline(steps=[('s', StandardScaler()), ('m', model)])\n",
    "    df_per_fold = pd.DataFrame(columns=['method', 'fold', 'mean','std', 'lower', 'upper'])\n",
    "    params = params_grid\n",
    "    counter = 0\n",
    "    scores = []\n",
    "    best_acc = 0\n",
    "    for train_ix, test_ix in tqdm(cv_outer.split(X_train, y_train)):\n",
    "        # split data\n",
    "        X_train_inner, X_test_inner = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "        y_train_inner, y_test_inner = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "\n",
    "        # define search\n",
    "        search = GridSearchCV(pipe, param_grid=params, scoring='accuracy', cv=cv_inner, n_jobs=-1)\n",
    "\n",
    "        # execute search\n",
    "        result = cross_val_score(search, X_train_inner, y_train_inner.values.ravel(), cv=cv_inner, n_jobs=-1)\n",
    "\n",
    "        scores.extend([result.mean()])\n",
    "        # df_per_fold = df_per_fold.append({'method': name, 'fold': counter, 'mean': result.mean(), 'std': result.std(), 'lower': result.mean() - result.std(), 'upper': result.mean() + result.std()}, ignore_index=True)\n",
    "        counter += 1\n",
    "        # check the best model\n",
    "        if result.mean() > best_acc:\n",
    "            best_acc = result.mean()\n",
    "            best_model = search\n",
    "\n",
    "    df_awnser = pd.concat([df, pd.DataFrame({'method': [name], 'mean': [np.mean(scores)], 'std': [np.std(scores)], 'lower': [np.mean(scores) - np.std(scores)], 'upper': [np.mean(scores) + np.std(scores)]})], ignore_index=True)\n",
    "    return df_awnser, df_per_fold\n",
    "\n",
    "def train_model(model,params_grid,name, df): \n",
    "    scalar = StandardScaler()\n",
    "    pipe = Pipeline(steps=[('s',scalar), ('m', model)])\n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid=params_grid, scoring='accuracy', cv=4, n_jobs=-1)\n",
    "\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "    scores = cross_val_score(gs, X_train, y_train.values.ravel(), scoring='accuracy', cv=rkf, n_jobs=-1)\n",
    "\n",
    "    df_awnser = pd.concat([df, pd.DataFrame({'method': [name], 'mean': [np.mean(scores)], 'std': [np.std(scores)], 'lower': [np.mean(scores) - np.std(scores)], 'upper': [np.mean(scores) + np.std(scores)]})], ignore_index=True)\n",
    "    return df_awnser, \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "bg = BaggingClassifier(n_estimators=3)\n",
    "\n",
    "name = 'BA'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,21]\n",
    "    } \n",
    "\n",
    "df, df_per_fold = train_model(bg,params_grid,name,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=3)\n",
    "\n",
    "name = 'AB'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,21]\n",
    "    }\n",
    "\n",
    "df,df_per_fold = train_model(ada,params_grid,name,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "name = 'RF'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,21]\n",
    "} \n",
    "\n",
    "\n",
    "df,df_per_fold = train_model(rf,params_grid,name,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZR</td>\n",
       "      <td>0.164674</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.151058</td>\n",
       "      <td>0.178290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG</td>\n",
       "      <td>0.478798</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.466268</td>\n",
       "      <td>0.491328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>0.238849</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>0.225923</td>\n",
       "      <td>0.251775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.472211</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>0.451497</td>\n",
       "      <td>0.492924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA</td>\n",
       "      <td>0.495471</td>\n",
       "      <td>0.072447</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.567918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AB</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.037745</td>\n",
       "      <td>0.190274</td>\n",
       "      <td>0.265765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.497766</td>\n",
       "      <td>0.077391</td>\n",
       "      <td>0.420375</td>\n",
       "      <td>0.575157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.495048</td>\n",
       "      <td>0.094941</td>\n",
       "      <td>0.400107</td>\n",
       "      <td>0.589989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BA</td>\n",
       "      <td>0.482548</td>\n",
       "      <td>0.081194</td>\n",
       "      <td>0.401354</td>\n",
       "      <td>0.563743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZR</td>\n",
       "      <td>0.160145</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.145136</td>\n",
       "      <td>0.175154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BA</td>\n",
       "      <td>0.477717</td>\n",
       "      <td>0.094859</td>\n",
       "      <td>0.382858</td>\n",
       "      <td>0.572577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AB</td>\n",
       "      <td>0.272403</td>\n",
       "      <td>0.049311</td>\n",
       "      <td>0.223093</td>\n",
       "      <td>0.321714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.492210</td>\n",
       "      <td>0.104280</td>\n",
       "      <td>0.387930</td>\n",
       "      <td>0.596491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   method      mean       std     lower     upper\n",
       "0      ZR  0.164674  0.013616  0.151058  0.178290\n",
       "1      BG  0.478798  0.012530  0.466268  0.491328\n",
       "2     ADA  0.238849  0.012926  0.225923  0.251775\n",
       "3      RF  0.472211  0.020714  0.451497  0.492924\n",
       "4      BA  0.495471  0.072447  0.423024  0.567918\n",
       "5      AB  0.228019  0.037745  0.190274  0.265765\n",
       "6      RF  0.497766  0.077391  0.420375  0.575157\n",
       "7      RF  0.495048  0.094941  0.400107  0.589989\n",
       "8      BA  0.482548  0.081194  0.401354  0.563743\n",
       "9      ZR  0.160145  0.015009  0.145136  0.175154\n",
       "10     BA  0.477717  0.094859  0.382858  0.572577\n",
       "11     AB  0.272403  0.049311  0.223093  0.321714\n",
       "12     RF  0.492210  0.104280  0.387930  0.596491"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Knearst Neighbors Classifier, Gaussian Naive Bayes Classifier, and Decision Tree Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define uma função para treinar os classificadores individuais em um dado conjunto de treinamento e retornar uma lista deles\n",
    "def train_classifiers(X_train, y_train):\n",
    "    # cria uma lista vazia para armazenar os classificadores\n",
    "    classifiers = []\n",
    "    # treina os classificadores individuais e os adiciona à lista\n",
    "    nn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    nb = GaussianNB().fit(X_train, y_train)\n",
    "    dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    classifiers.extend([nn, nb, dt])\n",
    "    # retorna a lista de classificadores\n",
    "    return classifiers\n",
    "\n",
    "def sample_data(X_train, y_train, random_state):\n",
    "    # amostra as características com reposição e obtém os rótulos correspondentes\n",
    "    X_train_sampled = X_train.sample(frac=1, replace=True, random_state=random_state)\n",
    "    y_train_sampled = y_train.loc[X_train_sampled.index]\n",
    "    # retorna o conjunto de dados amostrado\n",
    "    return X_train_sampled, y_train_sampled\n",
    "\n",
    "def predict_hp(row, classifiers, class_order):\n",
    "    # cria um dicionário vazio para armazenar os votos para cada classe\n",
    "    votes = {}\n",
    "    # percorre os classificadores individuais no conjunto\n",
    "    for clf in classifiers:\n",
    "    # obtém a predição do classificador para o exemplo de teste e a armazena no dicionário de votos\n",
    "        pred = clf.predict(row.values.reshape(1,-1))[0]\n",
    "        votes[pred] = votes.get(pred, 0) + 1\n",
    "\n",
    "    # obtém a(s) classe(s) mais votada(s) e as armazena em uma lista\n",
    "    max_votes = max(votes.values())\n",
    "    most_voted_classes = [k for k,v in votes.items() if v == max_votes]\n",
    "    \n",
    "    hp_pred = None\n",
    "\n",
    "    # se houver mais de uma classe mais votada, quebra o empate usando a ordem das classes do conjunto de treinamento\n",
    "    if len(most_voted_classes) > 1:\n",
    "        for c in class_order:\n",
    "            if c in most_voted_classes:\n",
    "                print(\"GOOT HERE\")\n",
    "                hp_pred = c\n",
    "                break\n",
    "        if hp_pred is None:\n",
    "            hp_pred = most_voted_classes[0]\n",
    "    # caso contrário, retorna a classe mais votada como a predição do conjunto HP\n",
    "    else:\n",
    "        hp_pred = most_voted_classes[0]\n",
    "\n",
    "    # retorna a predição\n",
    "    return hp_pred\n",
    "\n",
    "# define uma função para avaliar o conjunto HP em um dado conjunto de teste\n",
    "def evaluate_hp(X_test,y_test, classifiers, class_order):\n",
    "\n",
    "    # cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "    hp_predictions = []\n",
    "    # percorre os exemplos de teste\n",
    "    for index, row in X_test.iterrows():\n",
    "        # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "        hp_pred = predict_hp(row, classifiers, class_order)\n",
    "        hp_predictions.append(hp_pred)\n",
    "    # avalia a acurácia do conjunto HP no conjunto de teste\n",
    "    hp_accuracy = accuracy_score(y_test, hp_predictions)\n",
    "    # retorna a acurácia\n",
    "    return hp_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of HP ensemble is 0.4167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_order = y_train.value_counts().index.tolist()\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "n_samples = 3\n",
    "\n",
    "# ciclo para treinar os classificadores individuais\n",
    "for i in range(n_samples):\n",
    "    # se for a primeira iteração, use os dados de treinamento originais\n",
    "    if i == 0:\n",
    "        X_train_current = X_train.copy()\n",
    "        y_train_current = y_train.copy()\n",
    "    # caso contrário, crie um novo conjunto de treinamento amostrando com reposição os dados originais usando a função sample_data\n",
    "    else:\n",
    "        X_train_current, y_train_current = sample_data(X_train, y_train, i)\n",
    "    \n",
    "    # treina os classificadores individuais nos dados de treinamento atuais usando a função train_classifiers e os estende à lista\n",
    "    classifiers.extend(train_classifiers(X_train_current, y_train_current))\n",
    "\n",
    "\n",
    "# cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "hp_predictions = []\n",
    "\n",
    "# faz o loop sobre os exemplos de teste\n",
    "for index, row in X_test.iterrows():\n",
    "    # predict the class of the test example using the predict_hp function and append it to the list # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "    hp_pred = predict_hp(row, classifiers, class_order)\n",
    "    hp_predictions.append(predict_hp(row, classifiers, class_order))\n",
    "\n",
    "# avalia a acurácia do conjunto HP no conjunto de teste\n",
    "hp_accuracy = accuracy_score(y_test, hp_predictions)\n",
    "print(f'The accuracy of HP ensemble is {hp_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df \n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base estimators\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class HeterogeneousEnsemble(BaseEstimator):\n",
    "    # define o construtor para o classificador\n",
    "    def __init__(self,n_samples=3):\n",
    "        \n",
    "        self.classifiers  =  [DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB()]\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def train_classifiers(self, X_train, y_train):\n",
    "        # cria uma lista vazia para armazenar os classificadores treinados\n",
    "        trained_classifiers = []\n",
    "        # faz o loop sobre os classificadores individuais\n",
    "        for clf in self.classifiers:\n",
    "            # treina o classificador no conjunto de treinamento atual\n",
    "            clf.fit(X_train, y_train)\n",
    "            # adiciona o classificador treinado à lista\n",
    "            trained_classifiers.append(clf)\n",
    "        # retorna a lista de classificadores treinados\n",
    "        return trained_classifiers\n",
    "\n",
    "    def sample_data(self,X_train, y_train, random_state):\n",
    "        # amostra as características com reposição e obtém os rótulos correspondentes\n",
    "        X_train_sampled = X_train.sample(frac=1, replace=True, random_state=random_state)\n",
    "        y_train_sampled = y_train.loc[X_train_sampled.index]\n",
    "        # retorna o conjunto de dados amostrado\n",
    "        return X_train_sampled, y_train_sampled\n",
    "\n",
    "\n",
    "    def predict_hp(self, X_test, classifiers, class_order):\n",
    "        # cria um dicionário para armazenar as predições de cada classificador\n",
    "        votes = {}\n",
    "        # faz o loop sobre os classificadores individuais\n",
    "        for clf in classifiers:\n",
    "            # prediz a classe do exemplo de teste usando o classificador atual\n",
    "            pred = clf.predict(X_test)[0]\n",
    "            # armazena a predição no dicionário\n",
    "            votes[pred] = votes.get(pred, 0) + 1\n",
    "\n",
    "        # obtém a(s) classe(s) mais votada(s) e as armazena em uma lista\n",
    "        max_votes = max(votes.values())\n",
    "        most_voted_classes = [k for k,v in votes.items() if v == max_votes]\n",
    "        \n",
    "        hp_pred = None\n",
    "\n",
    "        # se houver mais de uma classe mais votada, quebra o empate usando a ordem das classes do conjunto de treinamento\n",
    "        if len(most_voted_classes) > 1:\n",
    "            for c in class_order:\n",
    "                if c in most_voted_classes:\n",
    "                    hp_pred = c\n",
    "                    break\n",
    "            if hp_pred is None:\n",
    "                hp_pred = most_voted_classes[0]\n",
    "        # caso contrário, retorna a classe mais votada como a predição do conjunto HP\n",
    "        else:\n",
    "            hp_pred = most_voted_classes[0]\n",
    "\n",
    "        # retorna a predição\n",
    "        return hp_pred\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,X_train,y_train): \n",
    "        \n",
    "\n",
    "        classifiers = []\n",
    "        print(self.n_samples)\n",
    "        # ciclo para treinar os classificadores individuais\n",
    "        for i in range(self.n_samples):\n",
    "            # se for a primeira iteração, use os dados de treinamento originais\n",
    "            if i == 0:\n",
    "                X_train_current = X_train.copy()\n",
    "                y_train_current = y_train.copy()\n",
    "            # caso contrário, crie um novo conjunto de treinamento amostrando com reposição os dados originais usando a função sample_data\n",
    "            else:\n",
    "                X_train_current, y_train_current = self.sample_data(X_train, y_train, i)\n",
    "            \n",
    "            # treina os classificadores individuais nos dados de treinamento atuais usando a função train_classifiers e os estende à lista\n",
    "            classifiers.extend(self.train_classifiers(X_train_current, y_train_current))\n",
    "\n",
    "    def predict(self,X_test): \n",
    "        class_order = y_train.value_counts().index.tolist()\n",
    "        # cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "        hp_predictions = []\n",
    "        # faz o loop sobre os exemplos de teste\n",
    "        for index, row in X_test.iterrows():\n",
    "            # predict the class of the test example using the predict_hp function and append it to the list # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "            hp_pred = predict_hp(row, classifiers, class_order)\n",
    "            hp_predictions.append(predict_hp(row, classifiers, class_order))\n",
    "        return hp_predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "hp = HeterogeneousEnsemble(n_samples=3)\n",
    "hp.fit(X_train,y_train)\n",
    "hp_predictions = hp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n",
      "\n",
      "\n",
      "3\n",
      "3\n",
      "33\n",
      "\n",
      "3\n",
      "9\n",
      "3\n",
      "39\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "3\n",
      "9\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "39\n",
      "3\n",
      "9\n",
      "3\n",
      "\n",
      "3\n",
      "9\n",
      "3153\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "\n",
      "\n",
      "3\n",
      "3\n",
      "15\n",
      "9\n",
      "93\n",
      "\n",
      "15\n",
      "3\n",
      "15\n",
      "\n",
      "15\n",
      "9\n",
      "15\n",
      "153\n",
      "\n",
      "9\n",
      "15\n",
      "315\n",
      "3\n",
      "15\n",
      "21\n",
      "9\n",
      "3\n",
      "9\n",
      "915393\n",
      "\n",
      "9\n",
      "15\n",
      "21\n",
      "93\n",
      "915\n",
      "3915\n",
      "15\n",
      "1515\n",
      "3\n",
      "15\n",
      "9\n",
      "9\n",
      "21\n",
      "15\n",
      "21\n",
      "21\n",
      "21\n",
      "\n",
      "\n",
      "21\n",
      "921\n",
      "9\n",
      "21\n",
      "15\n",
      "\n",
      "\n",
      "21\n",
      "9915\n",
      "9\n",
      "21\n",
      "\n",
      "\n",
      "\n",
      "21\n",
      "21\n",
      "9\n",
      "15\n",
      "15\n",
      "\n",
      "\n",
      "21\n",
      "9\n",
      "21\n",
      "15\n",
      "15\n",
      "15\n",
      "9\n",
      "21\n",
      "9\n",
      "21\n",
      "9\n",
      "3\n",
      "21\n",
      "15\n",
      "9\n",
      "\n",
      "15\n",
      "15\n",
      "3\n",
      "15\n",
      "21\n",
      "9\n",
      "21\n",
      "921\n",
      "\n",
      "915\n",
      "15\n",
      "1515\n",
      "15\n",
      "15\n",
      "\n",
      "21\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "\n",
      "15\n",
      "9\n",
      "\n",
      "21\n",
      "15\n",
      "21\n",
      "\n",
      "21\n",
      "9\n",
      "21\n",
      "15\n",
      "21\n",
      "21\n",
      "93\n",
      "2121\n",
      "\n",
      "92121\n",
      "2115\n",
      "15\n",
      "\n",
      "\n",
      "9\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "9\n",
      "\n",
      "9\n",
      "21\n",
      "\n",
      "3\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "15\n",
      "21\n",
      "3\n",
      "3\n",
      "9\n",
      "15\n",
      "21\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "3\n",
      "3\n",
      "3\n",
      "993\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "3\n",
      "15\n",
      "3\n",
      "3\n",
      "3\n",
      "39\n",
      "3\n",
      "\n",
      "21\n",
      "15\n",
      "3\n",
      "21\n",
      "3\n",
      "9\n",
      "9\n",
      "3\n",
      "3\n",
      "21\n",
      "9\n",
      "9\n",
      "15\n",
      "9\n",
      "3\n",
      "153\n",
      "9\n",
      "9\n",
      "9\n",
      "3\n",
      "15\n",
      "21\n",
      "15\n",
      "3\n",
      "3\n",
      "9\n",
      "3\n",
      "9\n",
      "3\n",
      "9\n",
      "15\n",
      "9\n",
      "15\n",
      "999\n",
      "399\n",
      "\n",
      "9\n",
      "3\n",
      "9\n",
      "3\n",
      "3\n",
      "9\n",
      "15\n",
      "15\n",
      "9\n",
      "1515\n",
      "\n",
      "9\n",
      "15\n",
      "15\n",
      "9\n",
      "9\n",
      "15\n",
      "9\n",
      "3\n",
      "3\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "9\n",
      "315\n",
      "\n",
      "\n",
      "39\n",
      "21\n",
      "21\n",
      "3\n",
      "2115\n",
      "15\n",
      "15\n",
      "21\n",
      "21\n",
      "15\n",
      "21\n",
      "\n",
      "99\n",
      "15\n",
      "21\n",
      "\n",
      "21\n",
      "\n",
      "1521\n",
      "15\n",
      "39\n",
      "21\n",
      "\n",
      "915\n",
      "\n",
      "15\n",
      "9\n",
      "21\n",
      "21\n",
      "15\n",
      "21\n",
      "9\n",
      "21\n",
      "21\n",
      "\n",
      "15\n",
      "15915\n",
      "21\n",
      "\n",
      "21\n",
      "152121\n",
      "3\n",
      "9159\n",
      "21\n",
      "\n",
      "15\n",
      "21\n",
      "21\n",
      "\n",
      "21\n",
      "21\n",
      "\n",
      "15\n",
      "9\n",
      "\n",
      "21\n",
      "3\n",
      "9\n",
      "21\n",
      "21\n",
      "\n",
      "21\n",
      "159\n",
      "21\n",
      "\n",
      "21\n",
      "15\n",
      "9\n",
      "2121\n",
      "21\n",
      "\n",
      "\n",
      "21\n",
      "21\n",
      "21\n",
      "\n",
      "21\n",
      "21\n",
      "15\n",
      "3\n",
      "3\n",
      "15\n",
      "21\n",
      "9\n",
      "21\n",
      "9\n",
      "15\n",
      "21\n",
      "15\n",
      "21\n",
      "3\n",
      "21\n",
      "15\n",
      "3\n",
      "15\n",
      "15\n",
      "21\n",
      "\n",
      "159\n",
      "219\n",
      "21\n",
      "\n",
      "3\n",
      "2115\n",
      "3\n",
      "3\n",
      "3\n",
      "\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "9\n",
      "3\n",
      "3\n",
      "\n",
      "9\n",
      "\n",
      "33\n",
      "9\n",
      "15\n",
      "9\n",
      "9\n",
      "9\n",
      "21\n",
      "3\n",
      "3\n",
      "9\n",
      "\n",
      "3\n",
      "39\n",
      "915\n",
      "\n",
      "15\n",
      "933\n",
      "15\n",
      "9\n",
      "15\n",
      "21\n",
      "\n",
      "915\n",
      "153\n",
      "\n",
      "9\n",
      "15\n",
      "\n",
      "915\n",
      "1521\n",
      "3915\n",
      "9321\n",
      "\n",
      "\n",
      "\n",
      "21\n",
      "9\n",
      "921915\n",
      "1515\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21\n",
      "9\n",
      "\n",
      "21\n",
      "15\n",
      "921\n",
      "15\n",
      "\n",
      "15\n",
      "15\n",
      "9\n",
      "9\n",
      "2115\n",
      "151515\n",
      "\n",
      "21\n",
      "2115\n",
      "21\n",
      "21\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "321\n",
      "2115\n",
      "\n",
      "21\n",
      "\n",
      "21\n",
      "212121\n",
      "\n",
      "\n",
      "212121\n",
      "\n",
      "\n",
      "21\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 875, in fit\n    self._run_search(evaluate_candidates)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 1379, in _run_search\n    evaluate_candidates(ParameterGrid(self.param_grid))\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 852, in evaluate_candidates\n    _warn_or_raise_about_fit_failures(out, self.error_score)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 367, in _warn_or_raise_about_fit_failures\n    raise ValueError(all_fits_failed_message)\nValueError: \nAll the 16 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n16 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 382, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/tmp/ipykernel_1237/3568319265.py\", line 77, in fit\n  File \"/tmp/ipykernel_1237/3568319265.py\", line 25, in sample_data\nAttributeError: 'numpy.ndarray' object has no attribute 'sample'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb Cell 16\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHP\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m params_grid \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mm__n_samples\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m3\u001b[39m,\u001b[39m9\u001b[39m,\u001b[39m15\u001b[39m,\u001b[39m21\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     }\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m df,df_per_fold \u001b[39m=\u001b[39m train_model(hp,params_grid,name,df)\n",
      "\u001b[1;32m/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb Cell 16\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m gs \u001b[39m=\u001b[39m GridSearchCV(pipe, param_grid\u001b[39m=\u001b[39mparams_grid, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m rkf \u001b[39m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_repeats\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m36851234\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(gs, X_train, y_train\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mravel(), scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, cv\u001b[39m=\u001b[39;49mrkf, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m df_awnser \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: [name], \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m: [np\u001b[39m.\u001b[39mmean(scores)], \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m: [np\u001b[39m.\u001b[39mstd(scores)], \u001b[39m'\u001b[39m\u001b[39mlower\u001b[39m\u001b[39m'\u001b[39m: [np\u001b[39m.\u001b[39mmean(scores) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mstd(scores)], \u001b[39m'\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m'\u001b[39m: [np\u001b[39m.\u001b[39mmean(scores) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mstd(scores)]})], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/loren/Desktop/trabalho-IA/1.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df_awnser, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 875, in fit\n    self._run_search(evaluate_candidates)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 1379, in _run_search\n    evaluate_candidates(ParameterGrid(self.param_grid))\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\", line 852, in evaluate_candidates\n    _warn_or_raise_about_fit_failures(out, self.error_score)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 367, in _warn_or_raise_about_fit_failures\n    raise ValueError(all_fits_failed_message)\nValueError: \nAll the 16 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n16 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/lorenzo/.local/lib/python3.8/site-packages/sklearn/pipeline.py\", line 382, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/tmp/ipykernel_1237/3568319265.py\", line 77, in fit\n  File \"/tmp/ipykernel_1237/3568319265.py\", line 25, in sample_data\nAttributeError: 'numpy.ndarray' object has no attribute 'sample'\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hp = HeterogeneousEnsemble()\n",
    "\n",
    "name = 'HP'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_samples': [3,9,15,21]\n",
    "    }\n",
    "\n",
    "df,df_per_fold = train_model(hp,params_grid,name,df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
