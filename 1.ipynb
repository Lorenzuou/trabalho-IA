{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heterogeneous pooling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data will be stored in this dataframe, with the method name, mean accuracy, standard deviation, lower and upper bound\n",
    "df = pd.DataFrame(columns=['method', 'mean', 'std', 'lower', 'upper'])\n",
    "\n",
    "df_per_fold = pd.DataFrame(columns=['method', 'fold', 'mean','std', 'lower', 'upper'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X.csv')\n",
    "y = pd.read_csv('y.csv')\n",
    "\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1237/3420571984.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'method': 'ZR', 'mean': scores.mean(), 'std': scores.std(), 'lower': scores.mean() - scores.std(), 'upper': scores.mean() + scores.std()}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Zero Rule Baseline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "df = df.append({'method': 'ZR', 'mean': scores.mean(), 'std': scores.std(), 'lower': scores.mean() - scores.std(), 'upper': scores.mean() + scores.std()}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,params_grid,name, df): \n",
    "    cv_inner = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=36851234)\n",
    "    cv_outer = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "    pipe = Pipeline(steps=[('s', StandardScaler()), ('m', model)])\n",
    "    df_per_fold = pd.DataFrame(columns=['method', 'fold', 'mean','std', 'lower', 'upper'])\n",
    "    params = params_grid\n",
    "    counter = 0\n",
    "    scores = []\n",
    "    best_acc = 0\n",
    "    for train_ix, test_ix in tqdm(cv_outer.split(X_train, y_train)):\n",
    "        # split data\n",
    "        X_train_inner, X_test_inner = X_train.iloc[train_ix, :], X_train.iloc[test_ix, :]\n",
    "        y_train_inner, y_test_inner = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "\n",
    "        # define search\n",
    "        search = GridSearchCV(pipe, param_grid=params, scoring='accuracy', cv=cv_inner, n_jobs=-1)\n",
    "\n",
    "        # execute search\n",
    "        result = cross_val_score(search, X_train_inner, y_train_inner.values.ravel(), cv=cv_inner, n_jobs=-1)\n",
    "\n",
    "        scores.extend([result.mean()])\n",
    "        # df_per_fold = df_per_fold.append({'method': name, 'fold': counter, 'mean': result.mean(), 'std': result.std(), 'lower': result.mean() - result.std(), 'upper': result.mean() + result.std()}, ignore_index=True)\n",
    "        counter += 1\n",
    "        # check the best model\n",
    "        if result.mean() > best_acc:\n",
    "            best_acc = result.mean()\n",
    "            best_model = search\n",
    "\n",
    "    df_awnser = pd.concat([df, pd.DataFrame({'method': [name], 'mean': [np.mean(scores)], 'std': [np.std(scores)], 'lower': [np.mean(scores) - np.std(scores)], 'upper': [np.mean(scores) + np.std(scores)]})], ignore_index=True)\n",
    "    return df_awnser, df_per_fold\n",
    "\n",
    "def train_model(model,params_grid,name, df): \n",
    "    scalar = StandardScaler()\n",
    "    pipe = Pipeline(steps=[('s',scalar), ('m', model)])\n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid=params_grid, scoring='accuracy', cv=4, n_jobs=-1)\n",
    "\n",
    "    rkf = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234)\n",
    "\n",
    "    scores = cross_val_score(gs, X_train, y_train.values.ravel(), scoring='accuracy', cv=rkf, n_jobs=-1)\n",
    "\n",
    "    df_awnser = pd.concat([df, pd.DataFrame({'method': [name], 'mean': [np.mean(scores)], 'std': [np.std(scores)], 'lower': [np.mean(scores) - np.std(scores)], 'upper': [np.mean(scores) + np.std(scores)]})], ignore_index=True)\n",
    "    return df_awnser, \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "bg = BaggingClassifier(n_estimators=3)\n",
    "\n",
    "name = 'BA'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,12]\n",
    "    } \n",
    "\n",
    "df, df_per_fold = train_model(bg,params_grid,name,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=3)\n",
    "\n",
    "name = 'AB'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,12]\n",
    "    }\n",
    "\n",
    "df,df_per_fold = train_model(ada,params_grid,name,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "name = 'RF'\n",
    "\n",
    "params_grid = {\n",
    "    'm__n_estimators': [3,9,15,12]\n",
    "} \n",
    "\n",
    "\n",
    "df,df_per_fold = train_model(rf,params_grid,name,df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Knearst Neighbors Classifier, Gaussian Naive Bayes Classifier, and Decision Tree Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define uma função para treinar os classificadores individuais em um dado conjunto de treinamento e retornar uma lista deles\n",
    "def train_classifiers(X_train, y_train):\n",
    "    # cria uma lista vazia para armazenar os classificadores\n",
    "    classifiers = []\n",
    "    # treina os classificadores individuais e os adiciona à lista\n",
    "    nn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    nb = GaussianNB().fit(X_train, y_train)\n",
    "    dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    classifiers.extend([nn, nb, dt])\n",
    "    # retorna a lista de classificadores\n",
    "    return classifiers\n",
    "\n",
    "def sample_data(X_train, y_train, random_state):\n",
    "    # amostra as características com reposição e obtém os rótulos correspondentes\n",
    "    X_train_sampled = X_train.sample(frac=1, replace=True, random_state=random_state)\n",
    "    y_train_sampled = y_train.loc[X_train_sampled.index]\n",
    "    # retorna o conjunto de dados amostrado\n",
    "    return X_train_sampled, y_train_sampled\n",
    "\n",
    "def predict_hp(row, classifiers, class_order):\n",
    "    # cria um dicionário vazio para armazenar os votos para cada classe\n",
    "    votes = {}\n",
    "    # percorre os classificadores individuais no conjunto\n",
    "    for clf in classifiers:\n",
    "    # obtém a predição do classificador para o exemplo de teste e a armazena no dicionário de votos\n",
    "        pred = clf.predict(row.values.reshape(1,-1))[0]\n",
    "        votes[pred] = votes.get(pred, 0) + 1\n",
    "\n",
    "    # obtém a(s) classe(s) mais votada(s) e as armazena em uma lista\n",
    "    max_votes = max(votes.values())\n",
    "    most_voted_classes = [k for k,v in votes.items() if v == max_votes]\n",
    "    \n",
    "    hp_pred = None\n",
    "\n",
    "    # se houver mais de uma classe mais votada, quebra o empate usando a ordem das classes do conjunto de treinamento\n",
    "    if len(most_voted_classes) > 1:\n",
    "        for c in class_order:\n",
    "            if c in most_voted_classes:\n",
    "                print(\"GOOT HERE\")\n",
    "                hp_pred = c\n",
    "                break\n",
    "        if hp_pred is None:\n",
    "            hp_pred = most_voted_classes[0]\n",
    "    # caso contrário, retorna a classe mais votada como a predição do conjunto HP\n",
    "    else:\n",
    "        hp_pred = most_voted_classes[0]\n",
    "\n",
    "    # retorna a predição\n",
    "    return hp_pred\n",
    "\n",
    "# define uma função para avaliar o conjunto HP em um dado conjunto de teste\n",
    "def evaluate_hp(X_test,y_test, classifiers, class_order):\n",
    "\n",
    "    # cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "    hp_predictions = []\n",
    "    # percorre os exemplos de teste\n",
    "    for index, row in X_test.iterrows():\n",
    "        # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "        hp_pred = predict_hp(row, classifiers, class_order)\n",
    "        hp_predictions.append(hp_pred)\n",
    "    # avalia a acurácia do conjunto HP no conjunto de teste\n",
    "    hp_accuracy = accuracy_score(y_test, hp_predictions)\n",
    "    # retorna a acurácia\n",
    "    return hp_accuracy\n",
    "\n",
    "\n",
    "class_order = y_train.value_counts().index.tolist()\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "n_samples = 3\n",
    "\n",
    "# ciclo para treinar os classificadores individuais\n",
    "for i in range(n_samples):\n",
    "    # se for a primeira iteração, use os dados de treinamento originais\n",
    "    if i == 0:\n",
    "        X_train_current = X_train.copy()\n",
    "        y_train_current = y_train.copy()\n",
    "    # caso contrário, crie um novo conjunto de treinamento amostrando com reposição os dados originais usando a função sample_data\n",
    "    else:\n",
    "        X_train_current, y_train_current = sample_data(X_train, y_train, i)\n",
    "    \n",
    "    # treina os classificadores individuais nos dados de treinamento atuais usando a função train_classifiers e os estende à lista\n",
    "    classifiers.extend(train_classifiers(X_train_current, y_train_current))\n",
    "\n",
    "\n",
    "# cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "hp_predictions = []\n",
    "\n",
    "# faz o loop sobre os exemplos de teste\n",
    "for index, row in X_test.iterrows():\n",
    "    # predict the class of the test example using the predict_hp function and append it to the list # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "    hp_pred = predict_hp(row, classifiers, class_order)\n",
    "    hp_predictions.append(predict_hp(row, classifiers, class_order))\n",
    "\n",
    "# avalia a acurácia do conjunto HP no conjunto de teste\n",
    "hp_accuracy = accuracy_score(y_test, hp_predictions)\n",
    "print(f'The accuracy of HP ensemble is {hp_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of HP ensemble is 0.3833\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_order = y_train.value_counts().index.tolist()\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "n_samples = 3\n",
    "\n",
    "# ciclo para treinar os classificadores individuais\n",
    "for i in range(n_samples):\n",
    "    # se for a primeira iteração, use os dados de treinamento originais\n",
    "    if i == 0:\n",
    "        X_train_current = X_train.copy()\n",
    "        y_train_current = y_train.copy()\n",
    "    # caso contrário, crie um novo conjunto de treinamento amostrando com reposição os dados originais usando a função sample_data\n",
    "    else:\n",
    "        X_train_current, y_train_current = sample_data(X_train, y_train, i)\n",
    "    \n",
    "    # treina os classificadores individuais nos dados de treinamento atuais usando a função train_classifiers e os estende à lista\n",
    "    classifiers.extend(train_classifiers(X_train_current, y_train_current))\n",
    "\n",
    "\n",
    "# cria uma lista vazia para armazenar as predições do conjunto HP\n",
    "hp_predictions = []\n",
    "\n",
    "# faz o loop sobre os exemplos de teste\n",
    "for index, row in X_test.iterrows():\n",
    "    # predict the class of the test example using the predict_hp function and append it to the list # prediz a classe do exemplo de teste usando a função predict_hp e a adiciona à lista\n",
    "    hp_pred = predict_hp(row, classifiers, class_order)\n",
    "    hp_predictions.append(predict_hp(row, classifiers, class_order))\n",
    "\n",
    "# avalia a acurácia do conjunto HP no conjunto de teste\n",
    "hp_accuracy = accuracy_score(y_test, hp_predictions)\n",
    "print(f'The accuracy of HP ensemble is {hp_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df \n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base estimators\n",
    "\n",
    "class HeterogeneousEnsemble(BaseEstimator, ClassifierMixin):\n",
    "    # define o construtor para o classificador\n",
    "    def __init__(self, classifiers):\n",
    "        # armazena os classificadores individuais como um atributo da classe\n",
    "        self.classifiers = classifiers\n",
    "\n",
    "    # define o método de ajuste para o classificador\n",
    "    def fit(self, X, y):\n",
    "        # ajusta os classificadores individuais nos dados de treinamento\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "\n",
    "    # define o método de predição para o classificador\n",
    "    def predict(self, X):\n",
    "        # cria uma lista vazia para armazenar as predições dos classificadores individuais\n",
    "        predictions = []\n",
    "        # percorre os classificadores individuais\n",
    "        for clf in self.classifiers:\n",
    "            # faz a predição do classificador para o conjunto de teste e a adiciona à lista\n",
    "            predictions.append(clf.predict(X))\n",
    "        # retorna a predição do classificador que obteve a maior quantidade de votos\n",
    "        return mode(predictions)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZR</td>\n",
       "      <td>0.164674</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>0.151058</td>\n",
       "      <td>0.178290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BG</td>\n",
       "      <td>0.478798</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.466268</td>\n",
       "      <td>0.491328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA</td>\n",
       "      <td>0.238849</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>0.225923</td>\n",
       "      <td>0.251775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.472211</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>0.451497</td>\n",
       "      <td>0.492924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA</td>\n",
       "      <td>0.495471</td>\n",
       "      <td>0.072447</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.567918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AB</td>\n",
       "      <td>0.228019</td>\n",
       "      <td>0.037745</td>\n",
       "      <td>0.190274</td>\n",
       "      <td>0.265765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.497766</td>\n",
       "      <td>0.077391</td>\n",
       "      <td>0.420375</td>\n",
       "      <td>0.575157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method      mean       std     lower     upper\n",
       "0     ZR  0.164674  0.013616  0.151058  0.178290\n",
       "1     BG  0.478798  0.012530  0.466268  0.491328\n",
       "2    ADA  0.238849  0.012926  0.225923  0.251775\n",
       "3     RF  0.472211  0.020714  0.451497  0.492924\n",
       "4     BA  0.495471  0.072447  0.423024  0.567918\n",
       "5     AB  0.228019  0.037745  0.190274  0.265765\n",
       "6     RF  0.497766  0.077391  0.420375  0.575157"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
